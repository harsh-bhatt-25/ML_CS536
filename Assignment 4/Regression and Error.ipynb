{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding $\\hat{w}$ and $\\hat{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](1.jpg)\n",
    "![title](2.jpg)\n",
    "![title](3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding expectations and variances of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](4.jpg)\n",
    "![title](5.jpg)\n",
    "![title](6.jpg)\n",
    "![title](7.jpg)\n",
    "![title](8.jpg)\n",
    "![title](9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argue that in the limit, the error on $\\hat{w}$ and $\\hat{b}$ are approximately equal to the given variances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](10.jpg)\n",
    "![title](11.jpg)\n",
    "![title](12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proving recentering produces same error in $\\hat{w}$ but minimizes it in $\\hat{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](13.jpg)\n",
    "![title](14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proving Experimentally the estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing $\\hat{w}$, $\\hat{w}'$, $\\hat{b}$, and $\\hat{b}'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final arrays to store data from all iterations.\n",
    "\n",
    "class Regression:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Initializing lists for a particular iteration\n",
    "        self.m, self.w, self.b, self.sigma_square = 200, 1, 5, 0.1\n",
    "        self.y_val, self.x_dash = [], []\n",
    "        self.w_hat, self.b_hat = [], []\n",
    "        self.w_hat_dash, self.b_hat_dash = [], []\n",
    "\n",
    "    def get_xdash(self, x):\n",
    "        for i in x:\n",
    "            self.x_dash.append(i - 101)\n",
    "        return np.array(self.x_dash)\n",
    "\n",
    "    def get_y_values(self, x):\n",
    "        for i in x:\n",
    "            epsilon = np.random.normal(0, self.sigma_square)\n",
    "            self.y_val.append((self.w * i) + self.b + epsilon)\n",
    "        return np.array(self.y_val)\n",
    "\n",
    "    def get_w_hat(self, x, y):\n",
    "        sum_x, sum_y = np.mean(x), np.mean(y)\n",
    "        sum_numerator, sum_denominator = 0, 0\n",
    "        for i in range(self.m):\n",
    "            sum_numerator += ((x[i] - sum_x) * (y[i] - sum_y))\n",
    "            sum_denominator += ((x[i] - sum_x) ** 2)\n",
    "        self.w_hat.append(sum_numerator / sum_denominator)\n",
    "        return np.array(self.w_hat)\n",
    "\n",
    "    def get_w_hat_dash(self, x, y):\n",
    "        sum_x, sum_y = np.mean(x), np.mean(y)\n",
    "        for i in range(self.m):\n",
    "            sum_numerator = (x[i] - sum_x) * (y[i] - sum_y)\n",
    "            sum_denominator = (x[i] - sum_x) ** 2\n",
    "        self.w_hat_dash.append(sum_numerator / sum_denominator)\n",
    "        return np.array(self.w_hat_dash)\n",
    "\n",
    "    def get_b_hat(self, x, y):\n",
    "        sum_x, sum_y = np.mean(x), np.mean(y)\n",
    "        return np.array([sum_y - (self.w_hat[0] * sum_x)])\n",
    "\n",
    "    def get_b_hat_dash(self, x, y):\n",
    "        sum_x, sum_y = np.mean(x), np.mean(y)\n",
    "        return np.array([sum_y - (self.w_hat_dash[0] * sum_x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final w_hat is : [array([0.9934715])]\n",
      "\n",
      "The final w_hat_dash is : [array([1.01154212])]\n",
      "\n",
      "The final b_hat is : [array([5.66086116])]\n",
      "\n",
      "The final b_hat_dash is : [array([106.00044959])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_x, final_x_dash, final_y = [], [], []\n",
    "final_w_hat, final_w_hat_dash = [], []\n",
    "final_b_hat, final_b_hat_dash = [], []\n",
    "\n",
    "for i in range(1):\n",
    "    lr = Regression()\n",
    "    x_val = np.random.uniform(100, 102, 200)\n",
    "    final_x.append(x_val)\n",
    "\n",
    "    y = lr.get_y_values(x_val)\n",
    "    final_y.append(y)\n",
    "\n",
    "    x_dash = lr.get_xdash(x_val)\n",
    "    final_x_dash.append(x_dash)\n",
    "\n",
    "    w_hat = lr.get_w_hat(x_val, y)\n",
    "    final_w_hat.append(w_hat)\n",
    "\n",
    "    w_hat_dash = lr.get_w_hat_dash(x_dash, y)\n",
    "    final_w_hat_dash.append(w_hat_dash)\n",
    "\n",
    "    b_hat = lr.get_b_hat(x_val, y)\n",
    "    final_b_hat.append(b_hat)\n",
    "\n",
    "    b_hat_dash = lr.get_b_hat_dash(x_dash, y)\n",
    "    final_b_hat_dash.append(b_hat_dash)\n",
    "\n",
    "print(\"The final w_hat is : {}\\n\".format(final_w_hat))\n",
    "print(\"The final w_hat_dash is : {}\\n\".format(final_w_hat_dash))\n",
    "print(\"The final b_hat is : {}\\n\".format(final_b_hat))\n",
    "print(\"The final b_hat_dash is : {}\\n\".format(final_b_hat_dash))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing it for 1000 times and finding the expected values and variances of weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final w_hat is : [array([1.01006036]), array([0.98066514]), array([0.98803768]), array([0.99103945]), array([0.99055359]), array([1.02096254]), array([0.98917179]), array([1.0107103]), array([1.00819679]), array([0.99494314])]\n",
      "\n",
      "The final w_hat_dash is : [array([0.88611349]), array([1.24011287]), array([0.74500076]), array([1.07682903]), array([1.23886453]), array([1.00589548]), array([1.06452075]), array([0.96786359]), array([0.42280495]), array([1.03358353])]\n",
      "\n",
      "The final b_hat is : [array([3.98562128]), array([6.95191465]), array([6.19827003]), array([5.91124894]), array([5.94552887]), array([2.8875427]), array([6.08801742]), array([3.91539118]), array([4.16465544]), array([5.511373])]\n",
      "\n",
      "The final b_hat_dash is : [array([106.00295571]), array([105.9943962]), array([105.98092248]), array([106.00030891]), array([105.98517083]), array([106.00388992]), array([105.99109597]), array([105.99898609]), array([106.02278977]), array([106.0028361])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_x, final_x_dash, final_y = [], [], []\n",
    "final_w_hat, final_w_hat_dash = [], []\n",
    "final_b_hat, final_b_hat_dash = [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    lr = Regression()\n",
    "    x_val = np.random.uniform(100, 102, 200)\n",
    "    final_x.append(x_val)\n",
    "\n",
    "    y = lr.get_y_values(x_val)\n",
    "    final_y.append(y)\n",
    "\n",
    "    x_dash = lr.get_xdash(x_val)\n",
    "    final_x_dash.append(x_dash)\n",
    "\n",
    "    w_hat = lr.get_w_hat(x_val, y)\n",
    "    final_w_hat.append(w_hat)\n",
    "\n",
    "    w_hat_dash = lr.get_w_hat_dash(x_dash, y)\n",
    "    final_w_hat_dash.append(w_hat_dash)\n",
    "\n",
    "    b_hat = lr.get_b_hat(x_val, y)\n",
    "    final_b_hat.append(b_hat)\n",
    "\n",
    "    b_hat_dash = lr.get_b_hat_dash(x_dash, y)\n",
    "    final_b_hat_dash.append(b_hat_dash)\n",
    "\n",
    "print(\"The final w_hat is : {}\\n\".format(final_w_hat[:10]))\n",
    "print(\"The final w_hat_dash is : {}\\n\".format(final_w_hat_dash[:10]))\n",
    "print(\"The final b_hat is : {}\\n\".format(final_b_hat[:10]))\n",
    "print(\"The final b_hat_dash is : {}\\n\".format(final_b_hat_dash[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding  the expected values of $\\hat{w}$, $\\hat{b}$, $\\hat{w}'$, and  $\\hat{b}'$ {-}\n",
    "\n",
    "$E[\\hat{w}] = w$\n",
    "So, $E[\\hat{w}] = 1$ as we are given the value of w as 1.\n",
    "\n",
    "$E[\\hat{b}] = b$\n",
    "So, $E[\\hat{b}] = 5$ (Given)\n",
    "\n",
    "Now we will see what are the values of the expectations we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of the expected value of w_hat is 0.9993226130808428\n",
      "\n",
      "Estimation of the expected value of w_hat' is 1.0859801848187285\n",
      "\n",
      "Estimation of the expected value of b_hat is 5.068158752294422\n",
      "\n",
      "Estimation of the expected value of b_hat' is 106.00192979197905\n"
     ]
    }
   ],
   "source": [
    "# Taking the mean of w_hat, w_hat', b_hat', and b_hat for all 1000 iterations to get the estimation of the expected values.\n",
    "\n",
    "print(\"Estimation of the expected value of w_hat is {}\\n\".format(np.mean(final_w_hat)))\n",
    "print(\"Estimation of the expected value of w_hat' is {}\\n\".format(np.mean(final_w_hat_dash)))\n",
    "print(\"Estimation of the expected value of b_hat is {}\\n\".format(np.mean(final_b_hat)))\n",
    "print(\"Estimation of the expected value of b_hat' is {}\".format(np.mean(final_b_hat_dash)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the values for $\\hat{w}$ and $\\hat{w}'$ are almost similar and the are comparable to the original w that we were given. This result makes sense.\n",
    "\n",
    "Now, we can see that $\\hat{b}$ and $\\hat{b}'$ have a huge difference. This is because as we saw in question 4, recentering the data does not do anything to the weights but changes the value for $\\hat{b}$. So, this result is also in concordance with what we found in the previous questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding  the variances of $\\hat{w}$, $\\hat{b}$, $\\hat{w}'$, and  $\\hat{b}'$ {-}\n",
    "\n",
    "$var[\\hat{w}] = \\frac{\\sigma^2}{\\sum^m_{i=1}(x_i - \\bar{x})^2}$\n",
    "\n",
    "$var[\\hat{b}] = \\sigma^2(\\frac{1}{m} + \\frac{\\bar{x}^2}{\\sum^m_{i=1}(x_i - \\bar{x})^2})$\n",
    "\n",
    "Now let's see what are the values of variances we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean w_hat variance is 0.0015199179975486446 and mean w_hat' variance is 0.0015199179975486446\n",
      "\n",
      "Mean b_hat variance is 15.509969829077079 and mean b_hat' variance is 0.005002522825888445\n",
      "\n",
      "\n",
      "The differences in the variances of w_hat and w_hat' is: 0.0\n",
      "\n",
      "The differences in the variances of b_hat and b_hat' is: 15.50496730625119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Taking the variance of w_hat, w_hat', b_hat', and b_hat for all 1000 iterations and taking a mean to see the final variance.\n",
    "'''\n",
    "var_w_hat, var_w_hat_dash, var_b_hat, var_b_hat_dash = [], [], [], []\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    i_sum, j_sum = 0, 0\n",
    "    mean_x_val, mean_x_dash = np.mean(final_x[i]), np.mean(final_x_dash[i])\n",
    "    \n",
    "    for j in range(200):\n",
    "        i_sum += ((final_x[i][j] - mean_x_val)**2)\n",
    "        j_sum += ((final_x_dash[i][j] - mean_x_dash)**2)\n",
    "        \n",
    "    current_var_w = lr.sigma_square / i_sum\n",
    "    var_w_hat.append(current_var_w)\n",
    "    var_b_hat.append((lr.sigma_square*0.05) + ((mean_x_val**2) * current_var_w))\n",
    "    \n",
    "    current_var_w_dash = lr.sigma_square / j_sum\n",
    "    var_w_hat_dash.append(current_var_w_dash)\n",
    "    var_b_hat_dash.append((lr.sigma_square*0.05) + ((mean_x_dash**2) * current_var_w_dash))\n",
    "\n",
    "\n",
    "print(\"Mean w_hat variance is {} and mean w_hat' variance is {}\\n\".format(np.mean(var_w_hat), np.mean(var_w_hat_dash)))\n",
    "print(\"Mean b_hat variance is {} and mean b_hat' variance is {}\\n\".format(np.mean(var_b_hat), np.mean(var_b_hat_dash)))\n",
    "print(\"\\nThe differences in the variances of w_hat and w_hat' is: {}\\n\".format\n",
    "      (abs(np.mean(var_w_hat)-np.mean(var_w_hat_dash))))\n",
    "print(\"The differences in the variances of b_hat and b_hat' is: {}\\n\".format\n",
    "      (abs(np.mean(var_b_hat)-np.mean(var_b_hat_dash))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the difference between variances of $\\hat{w}$ and $\\hat{w}'$ is 0. This is exactly what we saw in the previous questions that recentering the data does not affect the weights.\n",
    "\n",
    "For the variances of $\\hat{b}$ and $\\hat{b}'$, we can see that after recentering the data, the variance for $\\hat{b}'$ is decreased heavily. We proved this property in question 4 that after recentering the data, the var($\\hat{b}$') has a subtraction term $E[x^2] - \\mu^2$ which decreases the variance. This decrease is because before recentering, there was a huge difference in the eigenvalues as there was an uneven spread in one direction. But after recentering, that uneven spread was eliminated giving us relatively symmetric data around the origin. Hence, we also proved the recentering property experimentally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitively, why is there no change in the estimate of the slope when the data is shifted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](17a.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the figure, whenever we recenter the data, the angle at which the data cloud is situated does not change. This is because we just move the data cloud up and down according to the location of it's mean but we do not change the orientation of the data at all. In the equation of regression, $w$ represents the slope of the line and $b$ represents the y-intercept.\n",
    "\n",
    "So, whenever we shift the data, we only change the y-intercept values (as we are just moving the data cloud up and down in space) and $w$ values remain unchanged due to no alteration in the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proving that the condition number is minimized when we recenter by taking $\\mu = E[x]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](15.jpg)\n",
    "![title](16.jpg)\n",
    "![title](a.jpeg)\n",
    "![title](b.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}