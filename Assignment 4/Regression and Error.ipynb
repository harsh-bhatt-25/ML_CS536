{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final arrays to store data from all iterations.\n",
    "\n",
    "class Regression:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Initializing lists for a particular iteration\n",
    "        self.m, self.w, self.b, self.sigma_square = 200, 1, 5, 0.1\n",
    "        self.y_val, self.x_dash = [], []\n",
    "        self.w_hat, self.b_hat = [], []\n",
    "        self.w_hat_dash, self.b_hat_dash = [], []\n",
    "\n",
    "    def get_xdash(self, x):\n",
    "        for i in x:\n",
    "            self.x_dash.append(i - 101)\n",
    "        return np.array(self.x_dash)\n",
    "\n",
    "    def get_y_values(self, x):\n",
    "        for i in x:\n",
    "            epsilon = np.random.normal(0, self.sigma_square)\n",
    "            self.y_val.append((self.w * i) + self.b + epsilon)\n",
    "        return np.array(self.y_val)\n",
    "\n",
    "    def get_w_hat(self, x, y):\n",
    "        sum_x, sum_y = np.mean(x), np.mean(y)\n",
    "        sum_numerator, sum_denominator = 0, 0\n",
    "        for i in range(self.m):\n",
    "            sum_numerator += ((x[i] - sum_x) * (y[i] - sum_y))\n",
    "            sum_denominator += ((x[i] - sum_x) ** 2)\n",
    "        self.w_hat.append(sum_numerator / sum_denominator)\n",
    "        return np.array(self.w_hat)\n",
    "\n",
    "    def get_w_hat_dash(self, x, y):\n",
    "        sum_x, sum_y = np.mean(x), np.mean(y)\n",
    "        for i in range(self.m):\n",
    "            sum_numerator = (x[i] - sum_x) * (y[i] - sum_y)\n",
    "            sum_denominator = (x[i] - sum_x) ** 2\n",
    "        self.w_hat_dash.append(sum_numerator / sum_denominator)\n",
    "        return np.array(self.w_hat_dash)\n",
    "\n",
    "    def get_b_hat(self, x, y):\n",
    "        sum_x, sum_y = np.mean(x), np.mean(y)\n",
    "        return np.array([sum_y - (self.w_hat[0] * sum_x)])\n",
    "\n",
    "    def get_b_hat_dash(self, x, y):\n",
    "        sum_x, sum_y = np.mean(x), np.mean(y)\n",
    "        return np.array([sum_y - (self.w_hat_dash[0] * sum_x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final w_hat is : [array([1.00659993])]\n",
      "\n",
      "The final w_hat_dash is : [array([0.90066604])]\n",
      "\n",
      "The final b_hat is : [array([4.33559774])]\n",
      "\n",
      "The final b_hat_dash is : [array([106.0018886])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_x, final_x_dash, final_y = [], [], []\n",
    "final_w_hat, final_w_hat_dash = [], []\n",
    "final_b_hat, final_b_hat_dash = [], []\n",
    "\n",
    "for i in range(1):\n",
    "    lr = Regression()\n",
    "    x_val = np.random.uniform(100, 102, 200)\n",
    "    final_x.append(x_val)\n",
    "\n",
    "    y = lr.get_y_values(x_val)\n",
    "    final_y.append(y)\n",
    "\n",
    "    x_dash = lr.get_xdash(x_val)\n",
    "    final_x_dash.append(x_dash)\n",
    "\n",
    "    w_hat = lr.get_w_hat(x_val, y)\n",
    "    final_w_hat.append(w_hat)\n",
    "\n",
    "    w_hat_dash = lr.get_w_hat_dash(x_dash, y)\n",
    "    final_w_hat_dash.append(w_hat_dash)\n",
    "\n",
    "    b_hat = lr.get_b_hat(x_val, y)\n",
    "    final_b_hat.append(b_hat)\n",
    "\n",
    "    b_hat_dash = lr.get_b_hat_dash(x_dash, y)\n",
    "    final_b_hat_dash.append(b_hat_dash)\n",
    "\n",
    "print(\"The final w_hat is : {}\\n\".format(final_w_hat))\n",
    "print(\"The final w_hat_dash is : {}\\n\".format(final_w_hat_dash))\n",
    "print(\"The final b_hat is : {}\\n\".format(final_b_hat))\n",
    "print(\"The final b_hat_dash is : {}\\n\".format(final_b_hat_dash))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentally proving the previous proofs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing it for 1000 times and finding the expected values and variances of weights and biases. {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final w_hat is : [array([1.027454]), array([1.01286887]), array([0.98915036]), array([0.99106557]), array([0.97871476]), array([1.00605496]), array([0.97878403]), array([0.99744233]), array([1.01516377]), array([1.00340294])]\n",
      "\n",
      "The final w_hat_dash is : [array([0.84359317]), array([0.98192624]), array([2.63699047]), array([1.05262418]), array([1.34106426]), array([-0.63078116]), array([1.15004381]), array([1.16599413]), array([0.84649903]), array([0.97810231])]\n",
      "\n",
      "The final b_hat is : [array([2.22632136]), array([3.70140258]), array([6.10084248]), array([5.89929996]), array([7.15362844]), array([4.39055558]), array([7.15126144]), array([5.26094743]), array([3.46694312]), array([4.66235952])]\n",
      "\n",
      "The final b_hat_dash is : [array([105.99861878]), array([106.00189179]), array([106.01406294]), array([105.99932931]), array([106.01588354]), array([106.0362121]), array([106.00527454]), array([106.00356632]), array([106.00120281]), array([106.00406881])]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_x, final_x_dash, final_y = [], [], []\n",
    "final_w_hat, final_w_hat_dash = [], []\n",
    "final_b_hat, final_b_hat_dash = [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    lr = Regression()\n",
    "    x_val = np.random.uniform(100, 102, 200)\n",
    "    final_x.append(x_val)\n",
    "\n",
    "    y = lr.get_y_values(x_val)\n",
    "    final_y.append(y)\n",
    "\n",
    "    x_dash = lr.get_xdash(x_val)\n",
    "    final_x_dash.append(x_dash)\n",
    "\n",
    "    w_hat = lr.get_w_hat(x_val, y)\n",
    "    final_w_hat.append(w_hat)\n",
    "\n",
    "    w_hat_dash = lr.get_w_hat_dash(x_dash, y)\n",
    "    final_w_hat_dash.append(w_hat_dash)\n",
    "\n",
    "    b_hat = lr.get_b_hat(x_val, y)\n",
    "    final_b_hat.append(b_hat)\n",
    "\n",
    "    b_hat_dash = lr.get_b_hat_dash(x_dash, y)\n",
    "    final_b_hat_dash.append(b_hat_dash)\n",
    "\n",
    "print(\"The final w_hat is : {}\\n\".format(final_w_hat[:10]))\n",
    "print(\"The final w_hat_dash is : {}\\n\".format(final_w_hat_dash[:10]))\n",
    "print(\"The final b_hat is : {}\\n\".format(final_b_hat[:10]))\n",
    "print(\"The final b_hat_dash is : {}\\n\".format(final_b_hat_dash[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding  the expected values of $\\hat{w}$, $\\hat{b}$, $\\hat{w}'$, and  $\\hat{b}'$ {-}\n",
    "\n",
    "$E[\\hat{w}] = w$\n",
    "So, $E[\\hat{w}] = 1$ as we are given the value of w as 1.\n",
    "\n",
    "$E[\\hat{b}] = b$\n",
    "So, $E[\\hat{b}] = 5$ (Given)\n",
    "\n",
    "Now we will see what are the values of the expectations we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimation of the expected value of w_hat is 0.9995280944887417\n",
      "\n",
      "Estimation of the expected value of w_hat' is 0.96275031824227\n",
      "\n",
      "Estimation of the expected value of b_hat is 5.047626559386555\n",
      "\n",
      "Estimation of the expected value of b_hat' is 106.00023929946305\n"
     ]
    }
   ],
   "source": [
    "# Taking the mean of w_hat, w_hat', b_hat', and b_hat for all 1000 iterations to get the estimation of the expected values.\n",
    "\n",
    "print(\"Estimation of the expected value of w_hat is {}\\n\".format(np.mean(final_w_hat)))\n",
    "print(\"Estimation of the expected value of w_hat' is {}\\n\".format(np.mean(final_w_hat_dash)))\n",
    "print(\"Estimation of the expected value of b_hat is {}\\n\".format(np.mean(final_b_hat)))\n",
    "print(\"Estimation of the expected value of b_hat' is {}\".format(np.mean(final_b_hat_dash)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the values for $\\hat{w}$ and $\\hat{w}'$ are almost similar and the are comparable to the original w that we were given. This result makes sense.\n",
    "\n",
    "Now, we can see that $\\hat{b}$ and $\\hat{b}'$ have a huge difference. This is because as we saw in question 4, recentering the data does not do anything to the weights but changes the value of $\\hat{b}$ to $\\hat{w}'$. So, this result is also in concordance with what we found in the previous questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding  the variances of $\\hat{w}$, $\\hat{b}$, $\\hat{w}'$, and  $\\hat{b}'$ {-}\n",
    "\n",
    "$var[\\hat{w}] = \\frac{\\sigma^2}{\\sum^m_{i=1}(x_i - \\bar{x})^2}$\n",
    "\n",
    "$var[\\hat{b}] = \\sigma^2(\\frac{1}{m} + \\frac{\\bar{x}^2}{\\sum^m_{i=1}(x_i - \\bar{x})^2})$\n",
    "\n",
    "Now let's see what are the values of variances we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean w_hat variance is 0.001513820671501302 and mean w_hat' variance is 0.0015138206715013022\n",
      "\n",
      "Mean b_hat variance is 15.447272997213211 and mean b_hat' variance is 0.005002390768748216\n",
      "\n",
      "\n",
      "The differences in the variances of w_hat and w_hat' is: 2.168404344971009e-19\n",
      "\n",
      "The differences in the variances of b_hat and b_hat' is: 15.442270606444463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Taking the variance of w_hat, w_hat', b_hat', and b_hat for all 1000 iterations and taking a mean to see the final variance.\n",
    "'''\n",
    "var_w_hat, var_w_hat_dash, var_b_hat, var_b_hat_dash = [], [], [], []\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    i_sum, j_sum = 0, 0\n",
    "    mean_x_val, mean_x_dash = np.mean(final_x[i]), np.mean(final_x_dash[i])\n",
    "    \n",
    "    for j in range(200):\n",
    "        i_sum += ((final_x[i][j] - mean_x_val)**2)\n",
    "        j_sum += ((final_x_dash[i][j] - mean_x_dash)**2)\n",
    "        \n",
    "    current_var_w = lr.sigma_square / i_sum\n",
    "    var_w_hat.append(current_var_w)\n",
    "    var_b_hat.append((lr.sigma_square*0.05) + ((mean_x_val**2) * current_var_w))\n",
    "    \n",
    "    current_var_w_dash = lr.sigma_square / j_sum\n",
    "    var_w_hat_dash.append(current_var_w_dash)\n",
    "    var_b_hat_dash.append((lr.sigma_square*0.05) + ((mean_x_dash**2) * current_var_w_dash))\n",
    "\n",
    "\n",
    "print(\"Mean w_hat variance is {} and mean w_hat' variance is {}\\n\".format(np.mean(var_w_hat), np.mean(var_w_hat_dash)))\n",
    "print(\"Mean b_hat variance is {} and mean b_hat' variance is {}\\n\".format(np.mean(var_b_hat), np.mean(var_b_hat_dash)))\n",
    "print(\"\\nThe differences in the variances of w_hat and w_hat' is: {}\\n\".format\n",
    "      (abs(np.mean(var_w_hat)-np.mean(var_w_hat_dash))))\n",
    "print(\"The differences in the variances of b_hat and b_hat' is: {}\\n\".format\n",
    "      (abs(np.mean(var_b_hat)-np.mean(var_b_hat_dash))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the difference between variances of $\\hat{w}$ and $\\hat{w}'$ is minuscule. This is exactly what we saw in the previous questions.\n",
    "\n",
    "For the variances of $\\hat{b}$ and $\\hat{b}'$, we can see that after recentering the data, the variance for $\\hat{b}'$ is decreased heavily. We proved this property in question 4 that after recentering the data, the var($\\hat{b}$') has a subtraction term $E[x^2] - \\mu^2$ which decreases the variance. Hence, we also proved the recentering property experimentally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuitively, why is there no change in the estimate of the slope when the data is shifted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%%Attach figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the figure, whenever we recenter the data, the angle at which the data cloud is situated does not change. This is because we just move the data cloud up and down according to the location of it's mean but we do not change the orientation of the data at all. In the equation of regression, $w$ represents the slope of the line and $b$ represents the y-intercept.\n",
    "\n",
    "So, whenever we shift the data, we only change the y-intercept values (as we are just moving the data cloud up and down in space) and $w$ values remain unchanged due to no alteration in the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proving that the condition number is minimized when we recenter by taking $\\mu = E[x]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
